{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: <strong>Xingyi WANG</strong>  \n",
    "E-mail: <strong>xingyi.wang@student.ecp.fr</strong>  \n",
    "Github: <strong>https://github.com/StellaireXy/DeepLearning</strong>  \n",
    "  \n",
    "  \n",
    "RULES:\n",
    "\n",
    "* Do not create any additional cell \n",
    "<font color=#DC143C>-- SORRY, I created several cells in the very last part... otherwise it's too long to be readable</font>\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import urllib.request # this is useful when import the vectors directly from Internet via URL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data/\"\n",
    "PATH_TO_DATA_SST = PATH_TO_DATA + \"SST/\"\n",
    "PATH_TO_DATA_RES = PATH_TO_DATA + \"RES/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        if fname[0:4] == 'http':\n",
    "            self.load_wordvec_url(fname, nmax)\n",
    "        else:\n",
    "            self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "    \n",
    "    # Since I really don't want to download the 6GB file, I write this function to read directly from Internet.\n",
    "    # !!! Long live the aws server address !!!\n",
    "    def load_wordvec_url(self, flink, nmax):\n",
    "        self.word2vec = {}\n",
    "        with urllib.request.urlopen(flink) as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.decode('utf-8')\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i % 1000 == 0:\n",
    "                    print(i, end=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "    \n",
    "    # w could be either LIST(= vector) or STR(= word)\n",
    "    def most_similar(self, w, K=5):\n",
    "        all_list = []\n",
    "        for key in self.word2vec:\n",
    "            if key != w:\n",
    "                all_list.append((self.score(key, w), key))\n",
    "        topk = sorted(all_list, key=lambda x: x[0], reverse=True)[0:K]\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        return topk\n",
    "    \n",
    "    # The score function should accept both LIST(= vector) & STR(= word)\n",
    "    def score(self, w1, w2):\n",
    "        if type(w1) == str:\n",
    "            vw1 = self.word2vec[w1]\n",
    "        else:\n",
    "            vw1 = w1\n",
    "        if type(w2) == str:\n",
    "            vw2 = self.word2vec[w2]\n",
    "        else:\n",
    "            vw2 = w2\n",
    "        \n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        return (np.dot(vw1, vw2) / (np.linalg.norm(vw1) * np.linalg.norm(vw2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70000 pretrained word vectors\n",
      "cat\tdog\t 0.671683666279249\n",
      "dog\tpet\t 0.6842064029669219\n",
      "dogs\tcats\t 0.7074389328052403\n",
      "paris\tfrance\t 0.7775108541288561\n",
      "germany\tberlin\t 0.7420295235998392\n",
      "\n",
      "cat\n",
      "cats\t 0.8353184714264993\n",
      "kitty\t 0.8034410478493814\n",
      "kitten\t 0.8024762062392743\n",
      "feline\t 0.768065407691186\n",
      "kitties\t 0.7237089223394708\n",
      "\n",
      "dog\n",
      "dogs\t 0.855207916336258\n",
      "puppy\t 0.784569427961543\n",
      "Dog\t 0.7511571638004245\n",
      "doggie\t 0.744241335717672\n",
      "canine\t 0.7421250622701407\n",
      "\n",
      "dogs\n",
      "dog\t 0.855207916336258\n",
      "Dogs\t 0.7704396457434113\n",
      "doggies\t 0.7699192773615036\n",
      "canines\t 0.7527040042648147\n",
      "puppies\t 0.7135063992407494\n",
      "\n",
      "paris\n",
      "france\t 0.7775108541288561\n",
      "Paris\t 0.6845140397494099\n",
      "london\t 0.6728545431461278\n",
      "berlin\t 0.6424447628126261\n",
      "tokyo\t 0.6409621495653873\n",
      "\n",
      "germany\n",
      "europe\t 0.7597591231074469\n",
      "german\t 0.7445826305760618\n",
      "berlin\t 0.7420295235998392\n",
      "sweden\t 0.700162288191849\n",
      "france\t 0.6968344551813622\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=70000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1+'\\t'+w2+'\\t', w2v.score(w1, w2))\n",
    "\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(\"\\n\"+w1)\n",
    "    li = w2v.most_similar(w1)\n",
    "    for x in li:\n",
    "        print(x[1]+'\\t', x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "                \n",
    "        sentemb = []\n",
    "        dictionary = self.w2v.word2vec\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                sentemb.append(np.mean([dictionary[w] for w in sent.strip().split() if w in dictionary], axis=0))\n",
    "            else:\n",
    "                sentemb.append(np.mean([dictionary[w]*idf[w] for w in sent.strip().split() if w in dictionary], axis=0))\n",
    "        \n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        all_list = []\n",
    "        for sent in sentences:\n",
    "            if sent != s:\n",
    "                all_list.append((self.score(sent, s, idf), sent))\n",
    "        topk = sorted(all_list, key=lambda x: x[0], reverse=True)[0:K]\n",
    "        return topk\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        vec1 = self.encode([s1], idf).flatten()\n",
    "        vec2 = self.encode([s2], idf).flatten()\n",
    "        return (np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in sent.strip().split():\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
    "        for w in idf.keys():\n",
    "            idf[w] = max(1, np.log10(len(sentences) / (idf[w])))\n",
    "        \n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70000 pretrained word vectors\n",
      "\n",
      "BoV-mean:\n",
      "0.9170453357707757 \tan african american man smiling . \n",
      "0.8498556003875812 \ta little african american boy and girl looking up . \n",
      "0.82209722421572 \tan african american in sunglasses and a white tee-shirt smiles . \n",
      "0.8217139139540895 \tan afican american woman standing behind two small african american children . \n",
      "0.8207047384633509 \tan african american man is sitting . \n",
      "score: 0.5726258859719607\n",
      "\n",
      "BoV-idf:\n",
      "0.9222156774278677 \tan african american man smiling . \n",
      "0.8717942965938766 \tan african american man is sitting . \n",
      "0.8653518571064772 \ta little african american boy and girl looking up . \n",
      "0.8552291860045897 \tan afican american woman standing behind two small african american children . \n",
      "0.8467228995937571 \ta girl in black hat holding an african american baby . \n",
      "score: 0.47509279801053633\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=70000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with open(PATH_TO_DATA+\"sentences.txt\") as f:\n",
    "    for line in f:\n",
    "        sentences.append(line)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if False else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "# BoV-mean\n",
    "li = s2v.most_similar('' if not sentences else sentences[10], sentences)\n",
    "print(\"\\nBoV-mean:\")\n",
    "for x in li:\n",
    "    print(x[0], '\\t'+x[1], end='')\n",
    "print(\"score:\", s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13]))\n",
    "\n",
    "# BoV-idf\n",
    "li = s2v.most_similar('' if not sentences else sentences[10], sentences, idf)\n",
    "print(\"\\nBoV-idf:\")\n",
    "for x in li:\n",
    "    print(x[0], '\\t'+x[1], end='')\n",
    "print(\"score:\", s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 Loaded 5000 pretrained word vectors\n",
      "0 1000 2000 3000 4000 Loaded 5000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# This step need INTERNET !!!\n",
    "link_en = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\"\n",
    "link_fr = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\"\n",
    "\n",
    "# This may take 5 mins Hihi ~ enjoy\n",
    "nmax = 5000  # If you need to test more words, please increase the nmax value\n",
    "w2v_en = Word2vec(link_en, nmax)\n",
    "w2v_fr = Word2vec(link_fr, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# X french\n",
    "# Y english\n",
    "x = []\n",
    "y = []\n",
    "for w in w2v_en.word2vec:\n",
    "    if w in w2v_fr.word2vec:\n",
    "        x.append(list(w2v_fr.word2vec[w]))\n",
    "        y.append(list(w2v_en.word2vec[w]))\n",
    "\n",
    "X = np.array(x).transpose() # X.shape = (300, nb)\n",
    "Y = np.array(y).transpose() # Y.shape = (300, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "U, sigma, Vt = np.linalg.svd(np.dot(Y, X.transpose()))\n",
    "W = np.dot(U, Vt)\n",
    "W = np.mat(W) # W.shape = (300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English -> French:\n",
      "\n",
      "cat\n",
      "chat\t 0.5036641656006448\n",
      "animal\t 0.4144212918264081\n",
      "chien\t 0.40298795641515883\n",
      "animaux\t 0.3748716512741023\n",
      "ours\t 0.3657892349483665\n",
      "\n",
      "dog\n",
      "chien\t 0.578191315236235\n",
      "animal\t 0.5243694960864647\n",
      "chat\t 0.47475871350912624\n",
      "ours\t 0.447601782356557\n",
      "boy\t 0.42668903202625247\n",
      "\n",
      "French -> English:\n",
      "\n",
      "chat\n",
      "cat\t 0.5036641656006452\n",
      "dog\t 0.47475871350912613\n",
      "cats\t 0.42752152928096354\n",
      "dogs\t 0.3921750560907983\n",
      "animal\t 0.37668589726291807\n",
      "\n",
      "chien\n",
      "dog\t 0.5781913152362349\n",
      "dogs\t 0.48611437672985464\n",
      "animal\t 0.4531164945753318\n",
      "animals\t 0.40586421231283004\n",
      "cat\t 0.40298795641515833\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "print(\"English -> French:\")\n",
    "# If you want to test more words, please increase the nmax value !!!\n",
    "for w in ['cat', 'dog']:\n",
    "    print(\"\\n\"+w)\n",
    "    vw_y = w2v_en.word2vec[w]\n",
    "    # X = np.dot(W.I, Y)\n",
    "    vw_x = list(np.array(np.dot(W.I, vw_y).transpose()).flatten())\n",
    "    li = w2v_fr.most_similar(vw_x)\n",
    "    for x in li:\n",
    "        print(x[1]+'\\t', x[0])\n",
    "        \n",
    "print(\"\\nFrench -> English:\")\n",
    "# If you want to test more words, please increase the nmax value !!!\n",
    "for w in ['chat', 'chien']:\n",
    "    print(\"\\n\"+w)\n",
    "    vw_x = w2v_fr.word2vec[w]\n",
    "    # Y = np.dot(W, X)\n",
    "    vw_y = list(np.array(np.dot(W, vw_x).transpose()).flatten())\n",
    "    li = w2v_en.most_similar(vw_y)\n",
    "    for x in li:\n",
    "        print(x[1]+'\\t', x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "with open(PATH_TO_DATA_SST+\"stsa.fine.train\") as f:\n",
    "    for line in f:\n",
    "        y, x = line.strip().split(' ', 1)\n",
    "        x_train.append(x)\n",
    "        y_train.append(int(y))\n",
    "l_train = len(x_train)\n",
    "        \n",
    "x_dev = []\n",
    "y_dev = []\n",
    "with open(PATH_TO_DATA_SST+\"stsa.fine.dev\") as f:\n",
    "    for line in f:\n",
    "        y, x = line.strip().split(' ', 1)\n",
    "        x_dev.append(x)\n",
    "        y_dev.append(int(y))\n",
    "l_dev = len(x_dev)\n",
    "\n",
    "x_test = []\n",
    "with open(PATH_TO_DATA_SST+\"stsa.fine.test.X\") as f:\n",
    "    for line in f:\n",
    "        x_test.append(line)\n",
    "l_test = len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=70000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if False else s2v.build_idf(x_train+x_dev+x_test)\n",
    "\n",
    "x_train_bov = s2v.encode(x_train, idf)\n",
    "x_dev_bov = s2v.encode(x_dev, idf)\n",
    "x_test_bov = s2v.encode(x_test, idf)\n",
    "\n",
    "#x_train_bov = s2v.encode(x_train)\n",
    "#x_dev_bov = s2v.encode(x_dev)\n",
    "#x_test_bov = s2v.encode(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.421435 \n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', penalty='l2', max_iter=150, random_state=9, multi_class='multinomial')\n",
    "lr.fit(x_train_bov, y_train)\n",
    "print(\"score : %f \" % (lr.score(x_dev_bov, y_dev))) # score: 0.421435 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "y_pred = lr.predict(x_test_bov)\n",
    "with open(PATH_TO_DATA_RES+\"logreg_bov_y_test_sst.txt\", \"w\") as f:\n",
    "    for i in y_pred:\n",
    "        f.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.bis - BONUS !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.421435 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svm = SVC(kernel='linear')\n",
    "clf_svm.fit(x_train_bov, y_train)  \n",
    "\n",
    "print(\"score : %f \" % (clf_svm.score(x_dev_bov, y_dev))) #0.4214350590372389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4296094459582198\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xy_train = xgb.DMatrix(x_train_bov, label=y_train)\n",
    "xy_dev = xgb.DMatrix(x_dev_bov, label=y_dev)\n",
    "xy_test = xgb.DMatrix(x_test_bov)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['lambda'] = 3\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 5\n",
    "param['booster'] = 'gblinear'\n",
    "\n",
    "num_round = 100\n",
    "bst = xgb.train(param, xy_train, num_round)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_dev_pred = bst.predict(xy_dev)\n",
    "print(accuracy_score(y_dev_pred, y_dev)) #0.4287011807447775\n",
    "\n",
    "y_test_pred = bst.predict(xy_test)\n",
    "with open(PATH_TO_DATA_RES+\"xgboost_bov_y_test_sst.txt\", \"w\") as f:\n",
    "    for i in y_test_pred:\n",
    "        f.write(str(int(i))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "with open(PATH_TO_DATA_SST+\"stsa.fine.train\") as f:\n",
    "    for line in f:\n",
    "        y, x = line.strip().split(' ', 1)\n",
    "        x_train.append(x)\n",
    "        y_train.append(y)\n",
    "l_train = len(x_train)\n",
    "        \n",
    "x_dev = []\n",
    "y_dev = []\n",
    "with open(PATH_TO_DATA_SST+\"stsa.fine.dev\") as f:\n",
    "    for line in f:\n",
    "        y, x = line.strip().split(' ', 1)\n",
    "        x_dev.append(x)\n",
    "        y_dev.append(y)\n",
    "l_dev = len(x_dev)\n",
    "\n",
    "x_test = []\n",
    "with open(PATH_TO_DATA_SST+\"stsa.fine.test.X\") as f:\n",
    "    for line in f:\n",
    "        x_test.append(line)\n",
    "l_test = len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "from keras.preprocessing import text\n",
    "N = 50000\n",
    "max_len = 60\n",
    "\n",
    "x_train_oh = [text.one_hot(i, n=N, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n', lower=True, split=' ') for i in x_train]\n",
    "x_dev_oh = [text.one_hot(i, n=N, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n', lower=True, split=' ') for i in x_dev]\n",
    "x_test_oh = [text.one_hot(i, n=N, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n', lower=True, split=' ') for i in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "x_train_oh = keras.preprocessing.sequence.pad_sequences(x_train_oh, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "x_dev_oh = keras.preprocessing.sequence.pad_sequences(x_dev_oh, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "x_test_oh = keras.preprocessing.sequence.pad_sequences(x_test_oh, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_dev_cat = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "embed_dim  = 16  # word embedding dimension\n",
    "nhid       = 32  # number of hidden units in the LSTM\n",
    "vocab_size = N  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, input_length=max_len))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 16)            800000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 806,437\n",
      "Trainable params: 806,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif, optimizer=optimizer, metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "bs = 32\n",
    "n_epochs = 2\n",
    "\n",
    "history = model.fit(x_train_oh, y_train_cat, batch_size=bs, epochs=n_epochs, validation_data=(x_dev_oh, y_dev_cat)) # 38%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 8544 samples, validate on 1101 samples  \n",
    "Epoch 1/2  \n",
    "8544/8544 [==============================] - 19s 2ms/step - loss: 1.5713 - acc: 0.2802 - val_loss: 1.5508 - val_acc: 0.3333  \n",
    "Epoch 2/2  \n",
    "8544/8544 [==============================] - 17s 2ms/step - loss: 1.4184 - acc: 0.3927 - val_loss: 1.3943 - val_acc: 0.3951  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "y_pred = model.predict_classes(x_test_oh)\n",
    "with open(PATH_TO_DATA_RES+\"logreg_lstm_y_test_sst.txt\", \"w\") as f:\n",
    "    for i in y_pred:\n",
    "        f.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Innovate ! 50% is a just a dream ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# Load pre-trained Word2Vec\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 -- CNN + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the train/dev/test sets to Word2Vec\n",
    "# x_train/dev/test_w2v = [[max_len * [vector 300]], [max_len * [vector 300]], [max_len * [vector 300]], ...]\n",
    "\n",
    "max_len = 60\n",
    "x_train_w2v = np.zeros((l_train, max_len, 300))\n",
    "for i in range(l_train):\n",
    "    j = 0\n",
    "    li_x = x_train[i].strip().split()\n",
    "    lenth = min(len(li_x), max_len)\n",
    "    for k in range(lenth):\n",
    "        if li_x[k] not in '!\" #$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' and li_x[k] in w2v.word2vec:\n",
    "        #if li_x[k] in w2v.word2vec:\n",
    "            x_train_w2v[i][j] = w2v.word2vec[li_x[k]]\n",
    "            j += 1\n",
    "\n",
    "x_dev_w2v = np.zeros((l_dev, max_len, 300))\n",
    "for i in range(l_dev):\n",
    "    j = 0\n",
    "    li_x = x_dev[i].strip().split()\n",
    "    lenth = min(len(li_x), max_len)\n",
    "    for k in range(lenth):\n",
    "        if li_x[k] not in '!\" #$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' and li_x[k] in w2v.word2vec:\n",
    "        #if li_x[k] in w2v.word2vec:\n",
    "            x_dev_w2v[i][j] = w2v.word2vec[li_x[k]]\n",
    "            j += 1\n",
    "\n",
    "x_test_w2v = np.zeros((l_test, max_len, 300))\n",
    "for i in range(l_test):\n",
    "    j = 0\n",
    "    li_x = x_test[i].strip().split()\n",
    "    lenth = min(len(li_x), max_len)\n",
    "    for k in range(lenth):\n",
    "        if li_x[k] not in '!\" #$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' and li_x[k] in w2v.word2vec:\n",
    "        #if li_x[k] in w2v.word2vec:\n",
    "            x_test_w2v[i][j] = w2v.word2vec[li_x[k]]\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import load_model\n",
    "\n",
    "n_classes  = 5\n",
    "\n",
    "#Building the neural network\n",
    "# Something strange here... If I use Tokenizer + Embedding, the performence is much worse T-T\n",
    "model_CNN = Sequential()\n",
    "model_CNN.add(Conv1D(kernel_size=5, padding='valid', input_shape=(max_len, 300), filters=128, activation='relu'))\n",
    "model_CNN.add(MaxPooling1D(pool_size=5, strides=None, padding='valid'))\n",
    "model_CNN.add(Conv1D(kernel_size=5, padding='valid', filters=32, activation='relu'))\n",
    "model_CNN.add(MaxPooling1D(pool_size=5, strides=None, padding='valid'))\n",
    "model_CNN.add(Flatten())\n",
    "model_CNN.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 56, 128)           192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7, 32)             20512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 212,805\n",
      "Trainable params: 212,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loss_classif     =  'categorical_crossentropy'\n",
    "optimizer        =  'nadam'\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model_CNN.compile(loss=loss_classif, optimizer=optimizer, metrics=metrics_classif)\n",
    "print(model_CNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "n_epochs = 3\n",
    "history = model_CNN.fit(x_train_w2v, y_train_cat, batch_size=bs, epochs=n_epochs, validation_data=(x_dev_w2v, y_dev_cat)) # 42%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 8544 samples, validate on 1101 samples  \n",
    "Epoch 1/3  \n",
    "8544/8544 [==============================] - 19s 2ms/step - loss: 1.4068 - acc: 0.3754 - val_loss: 1.3103 - val_acc: 0.4133  \n",
    "Epoch 2/3  \n",
    "8544/8544 [==============================] - 19s 2ms/step - loss: 1.1403 - acc: 0.5232 - val_loss: 1.3765 - val_acc: 0.3960  \n",
    "Epoch 3/3  \n",
    "8544/8544 [==============================] - 19s 2ms/step - loss: 0.9006 - acc: 0.6571 - val_loss: 1.3286 - val_acc: 0.4414  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_CNN.save('model_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = load_model('model_CNN.h5')\n",
    "\n",
    "y_pred_CNN = model_CNN.predict_classes(x_test_w2v)\n",
    "with open(PATH_TO_DATA_RES+\"cnn_w2v_y_test_sst.txt\", \"w\") as f:\n",
    "    for i in y_pred_CNN:\n",
    "        f.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 -- LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Tokenizer to transform x into numbers, so that we can use pre-trained word2vec in the Embedding layer\n",
    "# (Thanks to my dear friend Adel, he inspired me a lot !!!)\n",
    "# please refer to https://keras.io/preprocessing/text/\n",
    "# Words will be numbered from 1, so that 0 can leave to padding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "x_all = x_train + x_dev + x_test\n",
    "tk = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n', lower=True, split=\" \")\n",
    "tk.fit_on_texts(x_all)\n",
    "x_all_tk = tk.texts_to_sequences(x_all)\n",
    "\n",
    "# Padding the short sentence with 0, all the x_train/dev/test have the same length\n",
    "max_len = 60\n",
    "x_all_tk = keras.preprocessing.sequence.pad_sequences(x_all_tk, max_len)\n",
    "x_train_tk = x_all_tk[0:l_train]\n",
    "x_dev_tk = x_all_tk[l_train:l_train + l_dev]\n",
    "x_test_tk = x_all_tk[l_train + l_dev:]\n",
    "\n",
    "# Creat the embedding matrix. It is a hashing table.\n",
    "embedding_matrix = np.zeros((len(tk.word_index)+1, 300))\n",
    "for w, i in tk.word_index.items():\n",
    "    if w in w2v.word2vec:\n",
    "        embedding_matrix[i] = w2v.word2vec[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import load_model\n",
    "\n",
    "# specify parameters of the neural network\n",
    "nhid       = 128     # number of hidden units in the LSTM\n",
    "n_classes  = 5       # number of classes\n",
    "\n",
    "# Build the model\n",
    "model_LSTM = Sequential() \n",
    "model_LSTM.add(Embedding(len(tk.word_index)+1, 300, weights=[embedding_matrix], trainable=False, input_length = max_len)) # not trainable!\n",
    "model_LSTM.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_LSTM.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 60, 300)           5352000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 5,572,293\n",
      "Trainable params: 220,293\n",
      "Non-trainable params: 5,352,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define compilation parameter\n",
    "loss_classif     =  'categorical_crossentropy'\n",
    "optimizer        =  'nadam' # nadam is better than adam\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model_LSTM.compile(loss=loss_classif, optimizer=optimizer, metrics=metrics_classif)\n",
    "print(model_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "n_epochs = 6\n",
    "history = model_LSTM.fit(x_train_tk, y_train_cat, batch_size=bs, epochs=n_epochs, validation_data=(x_dev_tk, y_dev_cat)) # 44%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 8544 samples, validate on 1101 samples  \n",
    "Epoch 1/6  \n",
    "8544/8544 [==============================] - 43s 5ms/step - loss: 1.3995 - acc: 0.3893 - val_loss: 1.3165 - val_acc: 0.4096  \n",
    "Epoch 2/6  \n",
    "8544/8544 [==============================] - 41s 5ms/step - loss: 1.2948 - acc: 0.4404 - val_loss: 1.3282 - val_acc: 0.4360  \n",
    "Epoch 3/6  \n",
    "8544/8544 [==============================] - 42s 5ms/step - loss: 1.2468 - acc: 0.4608 - val_loss: 1.2754 - val_acc: 0.4414  \n",
    "Epoch 4/6  \n",
    "8544/8544 [==============================] - 47s 5ms/step - loss: 1.2151 - acc: 0.4764 - val_loss: 1.2764 - val_acc: 0.4405  \n",
    "Epoch 5/6  \n",
    "8544/8544 [==============================] - 44s 5ms/step - loss: 1.1758 - acc: 0.5002 - val_loss: 1.2565 - val_acc: 0.4614  \n",
    "Epoch 6/6  \n",
    "8544/8544 [==============================] - 43s 5ms/step - loss: 1.1392 - acc: 0.5114 - val_loss: 1.2499 - val_acc: 0.4523  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_LSTM.save('model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = load_model('model_LSTM.h5')\n",
    "\n",
    "y_pred_LSTM = model_LSTM.predict_classes(x_test_tk)\n",
    "with open(PATH_TO_DATA_RES+\"lstm_w2v_y_test_sst.txt\", \"w\") as f:\n",
    "    for i in y_pred_LSTM:\n",
    "        f.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 -- Vive La Démocratie !!! Let's VOTE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTE = LR + LSTM_w2v + CNN_w2v + XGB\n",
    "# LR: logreg_bov_y_test_sst.txt * 42\n",
    "# LSTM_w2v: lstm_w2v_y_test_sst.txt * 44\n",
    "# CNN_w2v: cnn_w2v_y_test_sst.txt * 42\n",
    "# XGB: xgboost_bov_y_test_sst.txt * 43\n",
    "\n",
    "y_pred_lr = []\n",
    "with open(PATH_TO_DATA_RES+\"logreg_bov_y_test_sst.txt\") as f:\n",
    "    for line in f:\n",
    "        y_pred_lr.append(int(line))\n",
    "\n",
    "y_pred_lstm = []\n",
    "with open(PATH_TO_DATA_RES+\"lstm_w2v_y_test_sst.txt\") as f:\n",
    "    for line in f:\n",
    "        y_pred_lstm.append(int(line))\n",
    "\n",
    "y_pred_cnn = []\n",
    "with open(PATH_TO_DATA_RES+\"cnn_w2v_y_test_sst.txt\") as f:\n",
    "    for line in f:\n",
    "        y_pred_cnn.append(int(line))\n",
    "\n",
    "y_pred_xgb = []\n",
    "with open(PATH_TO_DATA_RES+\"xgboost_bov_y_test_sst.txt\") as f:\n",
    "    for line in f:\n",
    "        y_pred_xgb.append(int(line))\n",
    "\n",
    "y_pred_vote = []\n",
    "for i in range(l_test):\n",
    "    vote = [0.0] * 5\n",
    "    vote[y_pred_lr[i]] += 42\n",
    "    vote[y_pred_lstm[i]] += 44\n",
    "    vote[y_pred_cnn[i]] += 42\n",
    "    vote[y_pred_xgb[i]] += 43\n",
    "    v_max = -1\n",
    "    j_max = -1\n",
    "    for j in range(5):\n",
    "        if v_max < vote[j]:\n",
    "            v_max = vote[j]\n",
    "            j_max = j\n",
    "    y_pred_vote.append(j_max)\n",
    "\n",
    "with open(PATH_TO_DATA_RES+\"vote_y_test_sst.txt\", \"w\") as f:\n",
    "    for i in y_pred_vote:\n",
    "        f.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried with dev data, accuracy after vote reached 0.45049954586739327\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#print(accuracy_score(np.array(y_pred_vote), y_dev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
